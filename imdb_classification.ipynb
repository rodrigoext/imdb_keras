{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encontro DEVs\n",
    "\n",
    "Código apresentado na segunda parte do encontro.\n",
    "\n",
    "Exemplo de analise de sentimento através da classificação dos reviews retirados do IMDb sobre diversos filmes.\n",
    "\n",
    "A idéia geral da estratégia, é fazer um indexamento das palavras para um número identificador (token), para então, serem usadas com entrada da rede neural. A indexação é feita pela frequencia que a palavra aparece em todo o conjunto de dados, por exemplo, o número 4 representa a 4ª palavra que mais aparece nos reviews.\n",
    "\n",
    "Da mesma maneira, o mapeamento é feito para as palavras \"Positivo\" (1) ou \"Negativo\" (0), afim de indentificar sobre o sentimento exposto naquele review.\n",
    "\n",
    "Como convenção, o \"0\" não representa nenhuma palavra, mas usada para representar alguma palavras desconhecida. Com isso, quando aparecer o \"0\", a análise sobre o sentimento de determinado review não é afetada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ferramentas utilizadas:\n",
    "<p>Python 3.6\n",
    "<p>Tensorflow (pode ser o Theano também)\n",
    "<p>Keras\n",
    "<p>Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence as prep\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# Carrega o conjunto de dados, mas mantem somente as n palavras mais mencionadas (relevantes)\n",
    "# zerando o restante\n",
    "\n",
    "# As 1000 palavras mais relevantes\n",
    "top_words = 1000\n",
    "\n",
    "# Carrega os conjunto de dados de reviews\n",
    "# Cada review é classificado em Positivo (1) ou Negativo (0)\n",
    "# Contem 25,000 reviews\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# maximo de palavras que o review pode conter, preenchendo o restante com zero\n",
    "max_words = 125\n",
    "X_train = prep.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = prep.pad_sequences(X_test, maxlen=max_words)\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 125, 32)           32000     \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 123, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 61, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 59, 64)            6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 29, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 27, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 250)               32250     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 148,915\n",
      "Trainable params: 148,915\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Criando o modelo\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=10))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      " - 8s - loss: 0.6609 - acc: 0.5844 - val_loss: 0.6164 - val_acc: 0.6581\n",
      "Epoch 2/5\n",
      " - 2s - loss: 0.5905 - acc: 0.6838 - val_loss: 0.6014 - val_acc: 0.6679\n",
      "Epoch 3/5\n",
      " - 2s - loss: 0.5561 - acc: 0.7143 - val_loss: 0.6078 - val_acc: 0.6655\n",
      "Epoch 4/5\n",
      " - 2s - loss: 0.5118 - acc: 0.7488 - val_loss: 0.6614 - val_acc: 0.6426\n",
      "Epoch 5/5\n",
      " - 2s - loss: 0.4514 - acc: 0.7896 - val_loss: 0.7090 - val_acc: 0.6267\n",
      "Trainamento realizado em  16.52 segundos.\n",
      "Acurácia: 62.67%\n"
     ]
    }
   ],
   "source": [
    "# Treinando a rede\n",
    "start = time.time()\n",
    "# Função de treinamento da rede. \n",
    "# O objetivo é aproximar uma função de acordo com os reviews e suas respectivas avaliações (Positivo ou Negativo)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128, verbose=2)\n",
    "print(\"Trainamento realizado em %.2f segundos.\" % (time.time() - start))\n",
    "scores = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(\"Acurácia: %.2f%%\" % (scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pega o dicionário da palavra para cada índice\n",
    "word_to_id = keras.datasets.imdb.get_word_index()\n",
    "# Criar outro discionário a partir do anterior que mapaeia o índice a respectiva palavra\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'movie', 'very', 'well', 'written', 'and', 'i', 'love', 'it']\n",
      "dict_keys([10, 9, 52, 395, 70, 116, 2, 17, 49])\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 10,   9,  52, 395,  70, 116,   2,  17,  49,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parte de teste da rede neural\n",
    "# A partir de um novo review, o mapeamento para os respectivos índices de cada palavra\n",
    "# Após o mapeamento, o review pode ser submetivo para a rede para a análise\n",
    "\n",
    "#my_review = 'worse movie bad script i hate the actors and photography'\n",
    "my_review = 'good movie very well written and i love it'\n",
    "words = my_review.split(\" \")\n",
    "print(words)\n",
    "\n",
    "my_r = {key:value for key, value in id_to_word.items() if value in words}\n",
    "print(my_r.keys())\n",
    "my_rr = np.array(list(my_r.keys()))\n",
    "print(my_rr.size)\n",
    "\n",
    "embedding_matrix = np.zeros((1, max_words)).astype(int)\n",
    "embedding_matrix[0, 0:my_rr.size] = my_rr\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good\n",
      "Accuracy: 79.70%\n"
     ]
    }
   ],
   "source": [
    "score = model.predict(embedding_matrix)\n",
    "if (score[0] * 100) > 50.00:\n",
    "    print(\"Good\")\n",
    "    print(\"Accuracy: %.2f%%\" % (score[0] * 100))\n",
    "else:\n",
    "    print(\"Bad\")\n",
    "    print(\"Accuracy: %.2f%%\" % ((1-score[0]) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
